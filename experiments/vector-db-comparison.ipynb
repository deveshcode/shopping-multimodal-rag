{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database Comparison Analysis\n",
    "## Comparing Popular Vector Databases for Semantic Search\n",
    "\n",
    "### Introduction\n",
    "This notebook provides a comparative analysis of popular vector databases including Pinecone, AWS OpenSearch, Milvus, Qdrant, Chroma, and Weaviate. We'll implement a practical semantic search use case across these platforms to evaluate their performance, ease of use, and specific strengths.\n",
    "\n",
    "### Setup and Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Vector DB clients\n",
    "import pinecone\n",
    "from opensearchpy import OpenSearch\n",
    "from milvus import Milvus, IndexType, MetricType\n",
    "from qdrant_client import QdrantClient\n",
    "import chromadb\n",
    "import weaviate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Preparation\n",
    "We'll use a dataset of product descriptions for our comparison:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample product descriptions\n",
    "products = [\n",
    "    \"High-performance wireless gaming mouse with RGB lighting\",\n",
    "    \"Mechanical keyboard with Cherry MX switches\",\n",
    "    \"27-inch 4K HDR gaming monitor with 144Hz refresh rate\",\n",
    "    \"Noise-cancelling wireless headphones with 30-hour battery life\",\n",
    "    \"Ergonomic office chair with lumbar support\",\n",
    "    # Add more products as needed\n",
    "]\n",
    "\n",
    "# Initialize sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(products)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation for Each Database\n",
    "\n",
    "#### 1. Pinecone Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pinecone():\n",
    "    pinecone.init(\n",
    "        api_key=os.getenv('PINECONE_API_KEY'),\n",
    "        environment=os.getenv('PINECONE_ENV')\n",
    "    )\n",
    "    \n",
    "    # Create index if it doesn't exist\n",
    "    if 'product-search' not in pinecone.list_indexes():\n",
    "        pinecone.create_index('product-search', dimension=384)\n",
    "    \n",
    "    index = pinecone.Index('product-search')\n",
    "    \n",
    "    # Insert vectors\n",
    "    for i, (text, vector) in enumerate(zip(products, embeddings)):\n",
    "        index.upsert([(str(i), vector.tolist(), {'text': text})])\n",
    "    \n",
    "    return index\n",
    "\n",
    "def search_pinecone(index, query_vector, top_k=3):\n",
    "    results = index.query(query_vector.tolist(), top_k=top_k, include_metadata=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. AWS OpenSearch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_opensearch():\n",
    "    client = OpenSearch(\n",
    "        hosts=[{'host': os.getenv('OPENSEARCH_HOST'), 'port': 443}],\n",
    "        http_auth=(os.getenv('OPENSEARCH_USER'), os.getenv('OPENSEARCH_PASS')),\n",
    "        use_ssl=True,\n",
    "        verify_certs=True\n",
    "    )\n",
    "    \n",
    "    # Create index with mapping\n",
    "    index_name = 'product-search'\n",
    "    mapping = {\n",
    "        'mappings': {\n",
    "            'properties': {\n",
    "                'vector': {'type': 'knn_vector', 'dimension': 384},\n",
    "                'text': {'type': 'text'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    client.indices.create(index=index_name, body=mapping)\n",
    "    \n",
    "    # Insert documents\n",
    "    for i, (text, vector) in enumerate(zip(products, embeddings)):\n",
    "        doc = {\n",
    "            'vector': vector.tolist(),\n",
    "            'text': text\n",
    "        }\n",
    "        client.index(index=index_name, id=str(i), body=doc)\n",
    "    \n",
    "    return client\n",
    "\n",
    "def search_opensearch(client, query_vector, top_k=3):\n",
    "    query = {\n",
    "        'size': top_k,\n",
    "        'query': {\n",
    "            'knn': {\n",
    "                'vector': {\n",
    "                    'vector': query_vector.tolist(),\n",
    "                    'k': top_k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = client.search(index='product-search', body=query)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Milvus Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_milvus():\n",
    "    client = Milvus(host=os.getenv('MILVUS_HOST'), port=os.getenv('MILVUS_PORT'))\n",
    "    \n",
    "    # Create collection\n",
    "    collection_name = 'product_search'\n",
    "    dimension = 384\n",
    "    \n",
    "    collection_param = {\n",
    "        'collection_name': collection_name,\n",
    "        'dimension': dimension,\n",
    "        'index_file_size': 1024,\n",
    "        'metric_type': MetricType.IP\n",
    "    }\n",
    "    \n",
    "    client.create_collection(collection_param)\n",
    "    \n",
    "    # Insert vectors\n",
    "    status, ids = client.insert(collection_name=collection_name,\n",
    "                              records=embeddings.tolist(),\n",
    "                              ids=[i for i in range(len(products))])\n",
    "    \n",
    "    # Create index\n",
    "    index_param = {\n",
    "        'nlist': 16384\n",
    "    }\n",
    "    client.create_index(collection_name, IndexType.IVF_FLAT, index_param)\n",
    "    \n",
    "    return client\n",
    "\n",
    "def search_milvus(client, query_vector, top_k=3):\n",
    "    status, results = client.search(\n",
    "        collection_name='product_search',\n",
    "        query_records=[query_vector.tolist()],\n",
    "        top_k=top_k,\n",
    "        params={'nprobe': 16}\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_performance_comparison(query_text: str = \"gaming peripheral with RGB\"):\n",
    "    results = {}\n",
    "    query_vector = model.encode([query_text])[0]\n",
    "    \n",
    "    # Test each database\n",
    "    dbs = {\n",
    "        'Pinecone': (setup_pinecone, search_pinecone),\n",
    "        'OpenSearch': (setup_opensearch, search_opensearch),\n",
    "        'Milvus': (setup_milvus, search_milvus),\n",
    "        'Qdrant': (setup_qdrant, search_qdrant),\n",
    "        'Chroma': (setup_chroma, search_chroma),\n",
    "        'Weaviate': (setup_weaviate, search_weaviate)\n",
    "    }\n",
    "    \n",
    "    for db_name, (setup_fn, search_fn) in dbs.items():\n",
    "        try:\n",
    "            # Setup\n",
    "            setup_start = time.time()\n",
    "            client = setup_fn()\n",
    "            setup_time = time.time() - setup_start\n",
    "            \n",
    "            # Search\n",
    "            search_start = time.time()\n",
    "            _ = search_fn(client, query_vector)\n",
    "            search_time = time.time() - search_start\n",
    "            \n",
    "            results[db_name] = {\n",
    "                'setup_time': setup_time,\n",
    "                'search_time': search_time,\n",
    "                'status': 'Success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[db_name] = {\n",
    "                'status': f'Failed: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "comparison_results = run_performance_comparison()\n",
    "\n",
    "# Create visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "successful_dbs = comparison_results[comparison_results['status'] == 'Success']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "successful_dbs['setup_time'].plot(kind='bar')\n",
    "plt.title('Setup Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "successful_dbs['search_time'].plot(kind='bar')\n",
    "plt.title('Search Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "Based on our implementation and testing:\n",
    "\n",
    "1. **Ease of Setup**:\n",
    "   - Pinecone: Simplest setup with minimal configuration\n",
    "   - AWS OpenSearch: Requires more complex configuration but integrates well with AWS\n",
    "   - Milvus: More setup overhead but offers fine-grained control\n",
    "\n",
    "2. **Performance**:\n",
    "   - Query latency comparisons (from our tests)\n",
    "   - Scalability considerations\n",
    "   - Memory usage patterns\n",
    "\n",
    "3. **Feature Comparison**:\n",
    "   - Real-time update capabilities\n",
    "   - Supported distance metrics\n",
    "   - Additional functionality (filtering, metadata storage)\n",
    "\n",
    "4. **Cost Considerations**:\n",
    "   - Managed vs self-hosted options\n",
    "   - Pricing models\n",
    "   - Operational overhead\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Each vector database has its strengths:\n",
    "\n",
    "- Pinecone: Best for quick deployment and scaling\n",
    "- AWS OpenSearch: Ideal for AWS-integrated applications\n",
    "- Milvus: Strong choice for custom deployments\n",
    "- Qdrant: Excellent for real-time updates\n",
    "- Chroma: Great for rapid prototyping\n",
    "- Weaviate: Powerful for complex data models with GraphQL\n",
    "\n",
    "The choice depends on specific use case requirements, infrastructure preferences, and scaling needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
